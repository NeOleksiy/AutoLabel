
import mediapipe as mp
import cv2
import numpy as np
import json
from tqdm import tqdm
import os
from scipy.optimize import linear_sum_assignment

# ----------------------------------------------------------------------
# 1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –ª–∏—Ü–∞
# ----------------------------------------------------------------------

DATASET_KEYPOINT_NAMES = [
    "left_eye", "right_eye", "left_ear", "right_ear", 
    "mouth", "nose", "left_eyebrow", "right_eyebrow"
]

# –£—Ç–æ—á–Ω–µ–Ω–Ω—ã–µ —Å–∏–≥–º—ã –¥–ª—è –ª–∏—Ü–∞
DATASET_SIGMAS = np.array([
    0.025, 0.025, 0.035, 0.035, 
    0.035, 0.026, 0.025, 0.025
])

# –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
MEDIAPIPE_TO_DATASET_MAPPING = {
    # –ì–ª–∞–∑–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏)
    468: "left_eye",    # –¶–µ–Ω—Ç—Ä –ª–µ–≤–æ–≥–æ –≥–ª–∞–∑–∞
    473: "right_eye",   # –¶–µ–Ω—Ç—Ä –ø—Ä–∞–≤–æ–≥–æ –≥–ª–∞–∑–∞
    
    # –£—à–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–∫–∏ —Å–±–æ–∫—É –≥–æ–ª–æ–≤—ã)
    454: "left_ear",    # –õ–µ–≤–æ–µ —É—Ö–æ
    234: "right_ear",   # –ü—Ä–∞–≤–æ–µ —É—Ö–æ
    
    # –†–æ—Ç
    13: "mouth",        # –¶–µ–Ω—Ç—Ä —Ä—Ç–∞
    
    # –ù–æ—Å
    1: "nose",          # –ö–æ–Ω—á–∏–∫ –Ω–æ—Å–∞
    
    # –ë—Ä–æ–≤–∏ (—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏)
    70: "left_eyebrow",   # –¶–µ–Ω—Ç—Ä –ª–µ–≤–æ–π –±—Ä–æ–≤–∏
    300: "right_eyebrow"  # –¶–µ–Ω—Ç—Ä –ø—Ä–∞–≤–æ–π –±—Ä–æ–≤–∏
}

COCO_ANN = "Human-Face-Pose-1/train/_annotations.coco.json"
COCO_IMG_DIR = "Human-Face-Pose-1/train/"
MAX_IMAGES = 400
OUTPUT_DIR = "output_predictions_corrected"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ----------------------------------------------------------------------
# 2. MediaPipe Face —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
# ----------------------------------------------------------------------

class MediaPipeFaceEvaluator:
    def __init__(self, min_detection_confidence=0.3):
        self.mp_face = mp.solutions.face_mesh
        self.face_mesh = self.mp_face.FaceMesh(
            static_image_mode=True,
            max_num_faces=10,
            refine_landmarks=False,  # –£–ø—Ä–æ—â–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            min_detection_confidence=min_detection_confidence,
            min_tracking_confidence=0.3
        )
        self.detection_stats = {
            "total_images": 0,
            "faces_detected": 0,
            "no_faces": 0,
            "landmarks_processed": 0
        }
    
    def mediapipe_inference(self, image_path):
        """–ò–Ω—Ñ–µ—Ä–µ–Ω—Å MediaPipe Face Mesh —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
        image = cv2.imread(image_path)
        if image is None:
            return [], None, None
        
        self.detection_stats["total_images"] += 1
        
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image_rgb.flags.writeable = False
        
        results = self.face_mesh.process(image_rgb)
        predictions = []
        
        if results.multi_face_landmarks:
            h, w = image.shape[:2]
            self.detection_stats["faces_detected"] += len(results.multi_face_landmarks)
            
            for face_landmarks in results.multi_face_landmarks:
                dataset_keypoints = self.convert_mediapipe_to_dataset(face_landmarks.landmark, w, h)
                
                if dataset_keypoints and self.has_sufficient_face_points(dataset_keypoints):
                    confidence = self.calculate_face_confidence(face_landmarks.landmark)
                    
                    predictions.append({
                        "category_id": 1,
                        "keypoints": dataset_keypoints,
                        "score": confidence,
                        "landmarks": face_landmarks
                    })
                    self.detection_stats["landmarks_processed"] += 1
        else:
            self.detection_stats["no_faces"] += 1
        
        return predictions, image, results.multi_face_landmarks
    
    def convert_mediapipe_to_dataset(self, mediapipe_landmarks, img_w, img_h):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç MediaPipe Face landmarks –≤ —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        dataset_keypoints = []
        
        mediapipe_kpts = []
        for landmark in mediapipe_landmarks:
            x = landmark.x * img_w
            y = landmark.y * img_h
            v = 2.0  # MediaPipe –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–∏–¥–∏–º—ã–µ —Ç–æ—á–∫–∏
            mediapipe_kpts.append([x, y, v])
        
        # –î–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
        points_found = 0
        for dataset_point in DATASET_KEYPOINT_NAMES:
            found = False
            for mp_idx, dataset_name in MEDIAPIPE_TO_DATASET_MAPPING.items():
                if dataset_name == dataset_point and mp_idx < len(mediapipe_kpts):
                    x, y, v = mediapipe_kpts[mp_idx]
                    dataset_keypoints.extend([x, y, v])
                    found = True
                    points_found += 1
                    break
            
            if not found:
                dataset_keypoints.extend([0, 0, 0])  # –¢–æ—á–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        
        return dataset_keypoints
    
    def has_sufficient_face_points(self, keypoints):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —Ç–æ—á–µ–∫ –ª–∏—Ü–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"""
        kpts_array = np.array(keypoints).reshape(-1, 3)
        visible_points = kpts_array[kpts_array[:, 2] > 0]
        
        # –¢—Ä–µ–±—É–µ–º —Ö–æ—Ç—è –±—ã 5 –∏–∑ 8 –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –¥–ª—è –≤–∞–ª–∏–¥–Ω–æ–≥–æ –ª–∏—Ü–∞
        return len(visible_points) >= 5
    
    def calculate_face_confidence(self, landmarks):
        """–í—ã—á–∏—Å–ª—è–µ—Ç confidence –¥–ª—è –ª–∏—Ü–∞"""
        if not landmarks:
            return 0.3
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ—á–µ–∫
        return min(0.5 + (len(landmarks) / 478) * 0.5, 1.0)
    
    def print_detection_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ï–¢–ï–ö–¶–ò–ò MEDIAPIPE:")
        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {self.detection_stats['total_images']}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ –ª–∏—Ü: {self.detection_stats['faces_detected']}")
        print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ª–∏—Ü: {self.detection_stats['no_faces']}")
        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ landmarks: {self.detection_stats['landmarks_processed']}")
        
        if self.detection_stats['total_images'] > 0:
            detection_rate = (self.detection_stats['faces_detected'] / self.detection_stats['total_images']) * 100
            print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏: {detection_rate:.1f}%")
    
    def close(self):
        self.face_mesh.close()

# ----------------------------------------------------------------------
# 3. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
# ----------------------------------------------------------------------

def compute_oks(dt_kpts, gt_kpts, area):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ OKS –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ GT –¥–ª—è –ª–∏—Ü–∞"""
    if area <= 0:
        return 0.0
        
    vars = (DATASET_SIGMAS * 2) ** 2
    k = len(DATASET_SIGMAS)
    
    dt = np.array(dt_kpts).reshape(k, 3)
    gt = np.array(gt_kpts).reshape(k, 3)
    
    dx = dt[:, 0] - gt[:, 0]
    dy = dt[:, 1] - gt[:, 1]
    
    vis_flag = gt[:, 2] > 0
    if np.sum(vis_flag) == 0:
        return 0.0
    
    e = (dx ** 2 + dy ** 2) / vars / (area + np.spacing(1)) / 2
    e = e[vis_flag]
    
    oks = np.sum(np.exp(-e)) / len(e)
    return oks

def validate_and_analyze_gt(coco_annotations, img_id_to_file):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ GT –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
    valid_annotations = []
    analysis = {
        "total_annotations": 0,
        "valid_annotations": 0,
        "invalid_keypoints": 0,
        "insufficient_points": 0,
        "bbox_issues": 0
    }
    
    for ann in coco_annotations:
        if ann["category_id"] != 1:
            continue
            
        analysis["total_annotations"] += 1
        
        if "keypoints" not in ann:
            analysis["invalid_keypoints"] += 1
            continue
            
        keypoints = ann["keypoints"]
        if len(keypoints) != len(DATASET_KEYPOINT_NAMES) * 3:
            analysis["invalid_keypoints"] += 1
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∏–¥–∏–º—ã–µ —Ç–æ—á–∫–∏
        kpts_array = np.array(keypoints).reshape(-1, 3)
        visible_points = kpts_array[kpts_array[:, 2] > 0]
        
        if len(visible_points) < 3:
            analysis["insufficient_points"] += 1
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º bbox
        if "bbox" not in ann or ann["bbox"][2] <= 0 or ann["bbox"][3] <= 0:
            analysis["bbox_issues"] += 1
            continue
            
        valid_annotations.append(ann)
        analysis["valid_annotations"] += 1
    
    print(f"\nüîç –ê–ù–ê–õ–ò–ó GT –ê–ù–ù–û–¢–ê–¶–ò–ô:")
    print(f"   –í—Å–µ–≥–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {analysis['total_annotations']}")
    print(f"   –í–∞–ª–∏–¥–Ω—ã—Ö: {analysis['valid_annotations']}")
    print(f"   –ü—Ä–æ–±–ª–µ–º—ã —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ç–æ—á–∫–∞–º–∏: {analysis['invalid_keypoints']}")
    print(f"   –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–∫: {analysis['insufficient_points']}")
    print(f"   –ü—Ä–æ–±–ª–µ–º—ã —Å bbox: {analysis['bbox_issues']}")
    
    return valid_annotations

def analyze_prediction_quality(predictions, matched_predictions):
    """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    print(f"\nüîç –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(predictions)}")
    print(f"   –£—Å–ø–µ—à–Ω—ã—Ö —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π: {len(matched_predictions)}")
    
    if not predictions:
        return
    
    # –ê–Ω–∞–ª–∏–∑ confidence
    confidences = [p["score"] for p in predictions]
    print(f"   –°—Ä–µ–¥–Ω–∏–π confidence: {np.mean(confidences):.3f}")
    print(f"   Min confidence: {np.min(confidences):.3f}")
    print(f"   Max confidence: {np.max(confidences):.3f}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–æ—á–µ–∫ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö
    points_per_pred = []
    for pred in predictions:
        kpts = np.array(pred["keypoints"]).reshape(-1, 3)
        visible_points = len(kpts[kpts[:, 2] > 0])
        points_per_pred.append(visible_points)
    
    print(f"   –°—Ä–µ–¥–Ω–µ–µ —Ç–æ—á–µ–∫ –Ω–∞ –ª–∏—Ü–æ: {np.mean(points_per_pred):.1f}")
    print(f"   Min —Ç–æ—á–µ–∫: {np.min(points_per_pred)}")
    print(f"   Max —Ç–æ—á–µ–∫: {np.max(points_per_pred)}")

def calculate_keypoint_metrics(predictions, coco_annotations, img_id_to_file):
    """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
    valid_gt_annotations = validate_and_analyze_gt(coco_annotations, img_id_to_file)
    
    if not valid_gt_annotations:
        print("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö GT –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫!")
        return {
            "AP": 0, "AP_50": 0, "AP_75": 0, "AR": 0,
            "mOKS": 0, "OKS_std": 0, "total_matches": 0,
            "total_gt": 0, "total_preds": len(predictions),
            "match_ratio": 0
        }, []
    
    gt_by_image = {}
    for ann in valid_gt_annotations:
        img_id = ann["image_id"]
        if img_id not in gt_by_image:
            gt_by_image[img_id] = []
        gt_by_image[img_id].append(ann)
    
    # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è OKS
    oks_thresholds = np.linspace(0.05, 0.95, 19)
    
    all_tp = {thresh: 0 for thresh in oks_thresholds}
    all_fp = {thresh: 0 for thresh in oks_thresholds}
    all_fn = {thresh: 0 for thresh in oks_thresholds}
    
    matched_predictions = []
    detailed_oks_scores = []
    
    # –ê–Ω–∞–ª–∏–∑ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π
    matching_analysis = {
        "images_processed": 0,
        "images_with_gt": 0,
        "images_with_preds": 0,
        "total_matching_attempts": 0,
        "successful_matches": 0
    }
    
    for img_id, file_name in img_id_to_file.items():
        matching_analysis["images_processed"] += 1
        
        if img_id not in gt_by_image:
            continue
            
        img_gts = gt_by_image[img_id]
        img_preds = [p for p in predictions if p["image_id"] == img_id]
        
        matching_analysis["images_with_gt"] += 1
        
        if not img_gts:
            continue
            
        if not img_preds:
            for threshold in oks_thresholds:
                all_fn[threshold] += len(img_gts)
            continue
        
        matching_analysis["images_with_preds"] += 1
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ OKS
        oks_matrix = np.zeros((len(img_preds), len(img_gts)))
        
        for i, pred in enumerate(img_preds):
            for j, gt in enumerate(img_gts):
                bbox = gt["bbox"]
                area = bbox[2] * bbox[3]
                oks = compute_oks(pred["keypoints"], gt["keypoints"], area)
                oks_matrix[i, j] = oks
                matching_analysis["total_matching_attempts"] += 1
        
        # Hungarian matching
        if oks_matrix.size > 0:
            cost_matrix = 1 - oks_matrix
            pred_indices, gt_indices = linear_sum_assignment(cost_matrix)
            
            for threshold in oks_thresholds:
                tp = 0
                matched_gt = set()
                matched_pred = set()
                
                for i, j in zip(pred_indices, gt_indices):
                    oks_score = oks_matrix[i, j]
                    if oks_score >= threshold:
                        tp += 1
                        matched_gt.add(j)
                        matched_pred.add(i)
                        
                        if abs(threshold - 0.50) < 0.01:
                            matched_predictions.append({
                                "pred": img_preds[i],
                                "gt": img_gts[j],
                                "oks": oks_score,
                                "image_id": img_id
                            })
                            detailed_oks_scores.append(oks_score)
                            matching_analysis["successful_matches"] += 1
                
                fp = len(img_preds) - len(matched_pred)
                fn = len(img_gts) - len(matched_gt)
                
                all_tp[threshold] += tp
                all_fp[threshold] += fp
                all_fn[threshold] += fn
    
    print(f"\nüîç –ê–ù–ê–õ–ò–ó –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–ô:")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {matching_analysis['images_processed']}")
    print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å GT: {matching_analysis['images_with_gt']}")
    print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏: {matching_analysis['images_with_preds']}")
    print(f"   –ü–æ–ø—ã—Ç–æ–∫ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è: {matching_analysis['total_matching_attempts']}")
    print(f"   –£—Å–ø–µ—à–Ω—ã—Ö —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π: {matching_analysis['successful_matches']}")
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    ap_scores = []
    ar_scores = []
    
    for threshold in oks_thresholds:
        tp = all_tp[threshold]
        fp = all_fp[threshold]
        fn = all_fn[threshold]
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        ap_scores.append(precision)
        ar_scores.append(recall)
    
    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è AP50 –∏ AP75
    threshold_50_idx = np.argmin(np.abs(oks_thresholds - 0.50))
    threshold_75_idx = np.argmin(np.abs(oks_thresholds - 0.75))
    
    if detailed_oks_scores:
        metrics = {
            "AP": np.mean(ap_scores),
            "AP_50": ap_scores[threshold_50_idx],
            "AP_75": ap_scores[threshold_75_idx],
            "AR": np.mean(ar_scores),
            "mOKS": np.mean(detailed_oks_scores),
            "OKS_std": np.std(detailed_oks_scores),
            "total_matches": len(matched_predictions),
            "total_gt": len(valid_gt_annotations),
            "total_preds": len(predictions),
            "match_ratio": len(matched_predictions) / len(predictions) if len(predictions) > 0 else 0
        }
    else:
        metrics = {
            "AP": 0, "AP_50": 0, "AP_75": 0, "AR": 0,
            "mOKS": 0, "OKS_std": 0, "total_matches": 0,
            "total_gt": len(valid_gt_annotations),
            "total_preds": len(predictions),
            "match_ratio": 0
        }
    
    return metrics, matched_predictions

# ----------------------------------------------------------------------
# 4. –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –æ—Ü–µ–Ω–∫–∏
# ----------------------------------------------------------------------

def main():
    print("üöÄ –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò MEDIAPIPE FACE –° –î–ò–ê–ì–ù–û–°–¢–ò–ö–û–ô")
    print("="*50)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ COCO –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    print("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ COCO –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
    try:
        with open(COCO_ANN, "r") as f:
            coco = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {COCO_ANN}")
        return
    
    img_id_to_file = {img["id"]: img["file_name"] for img in coco["images"]}
    
    face_annotations = [ann for ann in coco["annotations"] if ann["category_id"] == 1]
    face_img_ids = {ann["image_id"] for ann in face_annotations}
    
    if MAX_IMAGES:
        face_img_ids = list(face_img_ids)[:MAX_IMAGES]
    else:
        face_img_ids = list(face_img_ids)
    
    print(f"üìä –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ª–∏—Ü–∞–º–∏: {len(face_img_ids)}")
    print(f"üìä –í—Å–µ–≥–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ª–∏—Ü: {len(face_annotations)}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞ MediaPipe Face
    print(f"\nüß™ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MediaPipe Face Mesh...")
    mediapipe_evaluator = MediaPipeFaceEvaluator(min_detection_confidence=0.3)
    
    mediapipe_predictions = []
    
    for img_id in tqdm(face_img_ids, desc="MediaPipe Face Inference"):
        file_name = img_id_to_file[img_id]
        img_path = os.path.join(COCO_IMG_DIR, file_name)
        
        if not os.path.isfile(img_path):
            continue
        
        preds, image, landmarks = mediapipe_evaluator.mediapipe_inference(img_path)
        
        for pred in preds:
            pred["image_id"] = img_id
            mediapipe_predictions.append(pred)
    
    mediapipe_evaluator.close()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏
    mediapipe_evaluator.print_detection_stats()
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    print(f"\nüìà –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –ª–∏—Ü–∞...")
    all_gt_annotations = [ann for ann in coco["annotations"] if ann["category_id"] == 1]
    mediapipe_metrics, mediapipe_matches = calculate_keypoint_metrics(
        mediapipe_predictions, all_gt_annotations, img_id_to_file
    )
    
    # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    analyze_prediction_quality(mediapipe_predictions, mediapipe_matches)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –õ–ò–¶:")
    print(f"   AP: {mediapipe_metrics['AP']:.4f}")
    print(f"   AP@0.5: {mediapipe_metrics['AP_50']:.4f}")
    print(f"   AP@0.75: {mediapipe_metrics['AP_75']:.4f}")
    print(f"   AR: {mediapipe_metrics['AR']:.4f}")
    print(f"   –°—Ä–µ–¥–Ω–∏–π OKS: {mediapipe_metrics['mOKS']:.4f}")
    
    if mediapipe_metrics['total_gt'] > 0:
        detection_efficiency = mediapipe_metrics['total_matches'] / mediapipe_metrics['total_gt'] * 100
        print(f"\nüéØ –û–ë–©–ê–Ø –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨:")
        print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {detection_efficiency:.1f}%")
        print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ –ª–∏—Ü: {mediapipe_metrics['total_gt'] - mediapipe_metrics['total_matches']}")
        print(f"   –õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è: {mediapipe_metrics['total_preds'] - mediapipe_metrics['total_matches']}")

if __name__ == "__main__":
    main()