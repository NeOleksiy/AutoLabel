import mediapipe as mp
import cv2
import numpy as np
import json
from tqdm import tqdm
import os
from scipy.optimize import linear_sum_assignment

# ----------------------------------------------------------------------
# 1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
# ----------------------------------------------------------------------

DATASET_KEYPOINT_NAMES = [
    "ear-left", "eye-left", "nose", "left-sh", "right-sh",
    "right-elb", "right-twist", "left-elb", "left-twist",
    "right-kan", "right-knee", "right-ankle", "left-kan",
    "left-knee", "left-ankle", "eye-right", "ear-right"
]

DATASET_SIGMAS = np.array([
    0.035, 0.025, 0.026, 0.079, 0.079,
    0.072, 0.062, 0.072, 0.062, 0.107,
    0.087, 0.089, 0.107, 0.087, 0.089,
    0.025, 0.035
])

MEDIAPIPE_TO_DATASET_MAPPING = {
    0: "nose", 2: "eye-left", 5: "eye-right", 7: "ear-left", 8: "ear-right",
    11: "left-sh", 12: "right-sh", 13: "left-elb", 14: "right-elb",
    15: "left-twist", 16: "right-twist", 23: "left-kan", 24: "right-kan",
    25: "left-knee", 26: "right-knee", 27: "left-ankle", 28: "right-ankle"
}

COCO_ANN = "human-pose-1/train/_annotations.coco.json"
COCO_IMG_DIR = "human-pose-1/train/"
MAX_IMAGES = 200

# ----------------------------------------------------------------------
# 2. MediaPipe Pose —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
# ----------------------------------------------------------------------

class MediaPipePoseEvaluator:
    def __init__(self, min_detection_confidence=0.3, model_complexity=1):
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=True,
            model_complexity=model_complexity,
            enable_segmentation=False,
            min_detection_confidence=min_detection_confidence
        )
    
    def mediapipe_inference(self, image_path):
        """–ò–Ω—Ñ–µ—Ä–µ–Ω—Å MediaPipe Pose –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        image = cv2.imread(image_path)
        if image is None:
            return [], None
        
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image_rgb.flags.writeable = False
        
        results = self.pose.process(image_rgb)
        predictions = []
        
        if results.pose_landmarks:
            h, w = image.shape[:2]
            dataset_keypoints = self.convert_mediapipe_to_dataset(results.pose_landmarks.landmark, w, h)
            
            if dataset_keypoints:
                visibilities = [lm.visibility for lm in results.pose_landmarks.landmark]
                confidence = np.mean(visibilities) if visibilities else 0.5
                
                predictions.append({
                    "category_id": 1,
                    "keypoints": dataset_keypoints,
                    "score": confidence
                })
        
        return predictions, image
    
    def convert_mediapipe_to_dataset(self, mediapipe_landmarks, img_w, img_h):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç MediaPipe landmarks –≤ —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        dataset_keypoints = []
        
        mediapipe_kpts = []
        for landmark in mediapipe_landmarks:
            x = landmark.x * img_w
            y = landmark.y * img_h
            v = 2.0 if landmark.visibility > 0.5 else 0.0
            mediapipe_kpts.append([x, y, v])
        
        for dataset_point in DATASET_KEYPOINT_NAMES:
            found = False
            for mp_idx, dataset_name in MEDIAPIPE_TO_DATASET_MAPPING.items():
                if dataset_name == dataset_point and mp_idx < len(mediapipe_kpts):
                    x, y, v = mediapipe_kpts[mp_idx]
                    dataset_keypoints.extend([x, y, v])
                    found = True
                    break
            
            if not found:
                dataset_keypoints.extend([0, 0, 0])
        
        return dataset_keypoints
    
    def close(self):
        self.pose.close()

# ----------------------------------------------------------------------
# 3. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫
# ----------------------------------------------------------------------

def compute_oks(dt_kpts, gt_kpts, area):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ OKS –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ GT"""
    if area <= 0:
        return 0.0
        
    vars = (DATASET_SIGMAS * 2) ** 2
    k = len(DATASET_SIGMAS)
    
    dt = np.array(dt_kpts).reshape(k, 3)
    gt = np.array(gt_kpts).reshape(k, 3)
    
    dx = dt[:, 0] - gt[:, 0]
    dy = dt[:, 1] - gt[:, 1]
    
    vis_flag = gt[:, 2] > 0
    if np.sum(vis_flag) == 0:
        return 0.0
    
    e = (dx ** 2 + dy ** 2) / vars / (area + np.spacing(1)) / 2
    e = e[vis_flag]
    
    oks = np.sum(np.exp(-e)) / len(e)
    return oks

def validate_gt_annotations(coco_annotations):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è GT –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π - –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫"""
    valid_annotations = []
    
    for ann in coco_annotations:
        if ann["category_id"] != 1:
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        if "keypoints" not in ann:
            print(f"‚ö†Ô∏è –í –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ {ann.get('id', 'unknown')} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏")
            continue
            
        keypoints = ann["keypoints"]
        if len(keypoints) != len(DATASET_KEYPOINT_NAMES) * 3:
            print(f"‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ {ann.get('id', 'unknown')}: "
                  f"–æ–∂–∏–¥–∞–ª–æ—Å—å {len(DATASET_KEYPOINT_NAMES) * 3}, –ø–æ–ª—É—á–µ–Ω–æ {len(keypoints)}")
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –≤–∏–¥–∏–º–∞—è –∫–ª—é—á–µ–≤–∞—è —Ç–æ—á–∫–∞
        kpts_array = np.array(keypoints).reshape(-1, 3)
        visible_points = kpts_array[kpts_array[:, 2] > 0]
        
        if len(visible_points) == 0:
            print(f"‚ö†Ô∏è –í –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ {ann.get('id', 'unknown')} –Ω–µ—Ç –≤–∏–¥–∏–º—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫")
            continue
            
        valid_annotations.append(ann)
    
    print(f"‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö GT –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(valid_annotations)}/{len(coco_annotations)}")
    return valid_annotations

def calculate_keypoint_metrics(predictions, coco_annotations, img_id_to_file):
    """
    –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º
    """
    # –í–∞–ª–∏–¥–∞—Ü–∏—è GT –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    valid_gt_annotations = validate_gt_annotations(coco_annotations)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º GT –ø–æ image_id
    gt_by_image = {}
    for ann in valid_gt_annotations:
        img_id = ann["image_id"]
        if img_id not in gt_by_image:
            gt_by_image[img_id] = []
        gt_by_image[img_id].append(ann)
    
    # –ü–æ—Ä–æ–≥–∏ –¥–ª—è OKS
    oks_thresholds = np.linspace(0.5, 0.95, 10)
    
    all_tp = {thresh: 0 for thresh in oks_thresholds}
    all_fp = {thresh: 0 for thresh in oks_thresholds}
    all_fn = {thresh: 0 for thresh in oks_thresholds}
    
    matched_predictions = []
    detailed_oks_scores = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    for img_id, file_name in img_id_to_file.items():
        if img_id not in gt_by_image:
            continue
            
        img_gts = gt_by_image[img_id]
        img_preds = [p for p in predictions if p["image_id"] == img_id]
        
        if not img_gts:
            continue
            
        if not img_preds:
            # –ï—Å–ª–∏ –µ—Å—Ç—å GT –Ω–æ –Ω–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π - —ç—Ç–æ FN
            for threshold in oks_thresholds:
                all_fn[threshold] += len(img_gts)
            continue
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ OKS –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä pred-gt
        oks_matrix = np.zeros((len(img_preds), len(img_gts)))
        
        for i, pred in enumerate(img_preds):
            for j, gt in enumerate(img_gts):
                bbox = gt["bbox"]
                area = bbox[2] * bbox[3]
                oks = compute_oks(pred["keypoints"], gt["keypoints"], area)
                oks_matrix[i, j] = oks
        
        # Hungarian matching –Ω–∞ –æ—Å–Ω–æ–≤–µ OKS
        cost_matrix = 1 - oks_matrix
        pred_indices, gt_indices = linear_sum_assignment(cost_matrix)
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ä–æ–≥–∞ —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        for threshold in oks_thresholds:
            tp = 0
            matched_gt = set()
            matched_pred = set()
            
            # True positives
            for i, j in zip(pred_indices, gt_indices):
                oks_score = oks_matrix[i, j]
                if oks_score >= threshold:
                    tp += 1
                    matched_gt.add(j)
                    matched_pred.add(i)
                    
                    if threshold == 0.5:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º matches –¥–ª—è –ø–æ—Ä–æ–≥–∞ 0.5
                        matched_predictions.append({
                            "pred": img_preds[i],
                            "gt": img_gts[j],
                            "oks": oks_score,
                            "image_id": img_id
                        })
                        detailed_oks_scores.append(oks_score)
            
            fp = len(img_preds) - len(matched_pred)
            fn = len(img_gts) - len(matched_gt)
            
            all_tp[threshold] += tp
            all_fp[threshold] += fp
            all_fn[threshold] += fn
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    ap_scores = []
    ar_scores = []
    
    for threshold in oks_thresholds:
        tp = all_tp[threshold]
        fp = all_fp[threshold]
        fn = all_fn[threshold]
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        ap_scores.append(precision)
        ar_scores.append(recall)
    
    if detailed_oks_scores:
        metrics = {
            "AP": np.mean(ap_scores),
            "AP_50": ap_scores[0],
            "AP_75": ap_scores[5],
            "AR": np.mean(ar_scores),
            "mOKS": np.mean(detailed_oks_scores),
            "OKS_std": np.std(detailed_oks_scores),
            "total_matches": len(matched_predictions),
            "total_gt": sum(len(gts) for gts in gt_by_image.values()),
            "total_preds": len(predictions),
            "match_ratio": len(matched_predictions) / len(predictions) if len(predictions) > 0 else 0
        }
    else:
        metrics = {
            "AP": 0, "AP_50": 0, "AP_75": 0, "AR": 0,
            "mOKS": 0, "OKS_std": 0, "total_matches": 0,
            "total_gt": sum(len(gts) for gts in gt_by_image.values()),
            "total_preds": len(predictions),
            "match_ratio": 0
        }
    
    return metrics, matched_predictions

def analyze_keypoint_performance(matched_predictions, metrics):
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ç–æ—á–∫–∞–º"""
    print(f"\nüîç –ê–ù–ê–õ–ò–ó –ö–õ–Æ–ß–ï–í–´–• –¢–û–ß–ï–ö:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {metrics['total_preds']}")
    print(f"   –í—Å–µ–≥–æ GT –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {metrics['total_gt']}")
    print(f"   –£—Å–ø–µ—à–Ω—ã—Ö —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π: {metrics['total_matches']}")
    print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è: {metrics['match_ratio']:.1%}")
    
    if not matched_predictions:
        return
    
    # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ OKS
    oks_scores = [m["oks"] for m in matched_predictions]
    good_matches = len([oks for oks in oks_scores if oks >= 0.5])
    excellent_matches = len([oks for oks in oks_scores if oks >= 0.75])
    
    print(f"\nüìä –ö–ê–ß–ï–°–¢–í–û –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–Ø:")
    print(f"   –°—Ä–µ–¥–Ω–∏–π OKS: {metrics['mOKS']:.3f} ¬± {metrics['OKS_std']:.3f}")
    print(f"   –•–æ—Ä–æ—à–∏–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è (OKS ‚â• 0.5): {good_matches}/{len(oks_scores)} ({good_matches/len(oks_scores):.1%})")
    print(f"   –û—Ç–ª–∏—á–Ω—ã–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è (OKS ‚â• 0.75): {excellent_matches}/{len(oks_scores)} ({excellent_matches/len(oks_scores):.1%})")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Ç–æ—á–∫–∞–º
    if matched_predictions:
        sample_match = matched_predictions[0]
        pred_kpts = np.array(sample_match["pred"]["keypoints"]).reshape(-1, 3)
        gt_kpts = np.array(sample_match["gt"]["keypoints"]).reshape(-1, 3)
        
        print(f"\nüìç –ü–†–ò–ú–ï–† –ö–õ–Æ–ß–ï–í–´–• –¢–û–ß–ï–ö (–ø–µ—Ä–≤–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ):")
        print(f"   OKS: {sample_match['oks']:.3f}")
        print(f"   –í–∏–¥–∏–º—ã—Ö —Ç–æ—á–µ–∫ –≤ GT: {np.sum(gt_kpts[:, 2] > 0)}")
        print(f"   –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ç–æ—á–µ–∫: {np.sum(pred_kpts[:, 2] > 0)}")

# ----------------------------------------------------------------------
# 4. –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –æ—Ü–µ–Ω–∫–∏
# ----------------------------------------------------------------------

def main():
    print("üöÄ –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò MEDIAPIPE POSE –ü–û –ö–õ–Æ–ß–ï–í–´–ú –¢–û–ß–ö–ê–ú")
    print("="*50)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ COCO –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    print("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ COCO –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
    try:
        with open(COCO_ANN, "r") as f:
            coco = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {COCO_ANN}")
        return
    except json.JSONDecodeError:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON —Ñ–∞–π–ª–∞: {COCO_ANN}")
        return
    
    img_id_to_file = {img["id"]: img["file_name"] for img in coco["images"]}
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –ª—é–¥–µ–π
    person_annotations = [ann for ann in coco["annotations"] if ann["category_id"] == 1]
    person_img_ids = {ann["image_id"] for ann in person_annotations}
    
    if MAX_IMAGES:
        person_img_ids = list(person_img_ids)[:MAX_IMAGES]
    else:
        person_img_ids = list(person_img_ids)
    
    print(f"üìä –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(person_img_ids)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ª—é–¥—å–º–∏")
    print(f"üìä –í—Å–µ–≥–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ª—é–¥–µ–π: {len(person_annotations)}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞ MediaPipe
    print(f"\nüß™ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MediaPipe Pose...")
    mediapipe_evaluator = MediaPipePoseEvaluator(
        min_detection_confidence=0.3,
        model_complexity=1
    )
    
    mediapipe_predictions = []
    processed_count = 0
    
    for img_id in tqdm(person_img_ids, desc="MediaPipe Inference"):
        file_name = img_id_to_file[img_id]
        img_path = os.path.join(COCO_IMG_DIR, file_name)
        
        if not os.path.isfile(img_path):
            print(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {img_path}")
            continue
        
        preds, image = mediapipe_evaluator.mediapipe_inference(img_path)
        
        for pred in preds:
            pred["image_id"] = img_id
            mediapipe_predictions.append(pred)
        
        processed_count += 1
    
    mediapipe_evaluator.close()
    
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {processed_count}/{len(person_img_ids)}")
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(mediapipe_predictions)}")
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫
    print(f"\nüìà –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫...")
    all_gt_annotations = [ann for ann in coco["annotations"] if ann["category_id"] == 1]
    mediapipe_metrics, mediapipe_matches = calculate_keypoint_metrics(
        mediapipe_predictions, all_gt_annotations, img_id_to_file
    )
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò:")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {processed_count}")
    print(f"   –ù–∞–π–¥–µ–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {mediapipe_metrics['total_preds']}")
    print(f"   –í—Å–µ–≥–æ GT –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {mediapipe_metrics['total_gt']}")
    print(f"   –£—Å–ø–µ—à–Ω—ã—Ö —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π: {mediapipe_metrics['total_matches']}")
    
    print(f"\nüìà –ú–ï–¢–†–ò–ö–ò –ö–õ–Æ–ß–ï–í–´–• –¢–û–ß–ï–ö:")
    print(f"   AP: {mediapipe_metrics['AP']:.4f}")
    print(f"   AP@0.5: {mediapipe_metrics['AP_50']:.4f}")
    print(f"   AP@0.75: {mediapipe_metrics['AP_75']:.4f}")
    print(f"   AR: {mediapipe_metrics['AR']:.4f}")
    print(f"   –°—Ä–µ–¥–Ω–∏–π OKS: {mediapipe_metrics['mOKS']:.4f}")
    print(f"   Std OKS: {mediapipe_metrics['OKS_std']:.4f}")
    print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è: {mediapipe_metrics['match_ratio']:.1%}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    analyze_keypoint_performance(mediapipe_matches, mediapipe_metrics)
    
    # –û–±—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    if mediapipe_metrics['total_gt'] > 0:
        detection_efficiency = mediapipe_metrics['total_matches'] / mediapipe_metrics['total_gt'] * 100
        print(f"\nüéØ –û–ë–©–ê–Ø –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨:")
        print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {detection_efficiency:.1f}%")
        print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ –ª—é–¥–µ–π: {mediapipe_metrics['total_gt'] - mediapipe_metrics['total_matches']}")
        print(f"   –õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è: {mediapipe_metrics['total_preds'] - mediapipe_metrics['total_matches']}")

if __name__ == "__main__":
    main()