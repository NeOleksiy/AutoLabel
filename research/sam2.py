import torch
from transformers import Sam2Model, Sam2Processor
from PIL import Image
import cv2
import numpy as np
import os
from pathlib import Path

class SAM2InferenceWithMetrics:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ SAM2 –∏–∑ transformers.
    """
    
    def __init__(self, model_name="facebook/sam2.1-hiera-large"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ SAM2.
        
        Args:
            model_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ SAM2 –Ω–∞ Hugging Face Hub.
        """
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ SAM2...")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = Sam2Model.from_pretrained(model_name).to(self.device)
        self.processor = Sam2Processor.from_pretrained(model_name)
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

    def get_bounding_boxes_from_masks(self, masks, original_size, confidence_threshold=0.5):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç bounding boxes –∏–∑ –º–∞—Å–æ–∫, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö SAM2.
        
        Args:
            masks: –í—ã—Ö–æ–¥–Ω—ã–µ –º–∞—Å–∫–∏ –æ—Ç –º–æ–¥–µ–ª–∏
            original_size: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            confidence_threshold: –ü–æ—Ä–æ–≥ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º–∞—Å–æ–∫ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            
        Returns:
            bounding_boxes: –°–ø–∏—Å–æ–∫ bounding boxes –≤ —Ñ–æ—Ä–º–∞—Ç–µ [x1, y1, x2, y2, score, class_id]
        """
        # –î–ª—è Sam2Model –∏–∑ transformers –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—ã—Ö–æ–¥—ã
        if isinstance(masks, torch.Tensor):
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –º–∞—Å–∫—É —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
            if masks.shape[1] > 0:
                mask = masks[0, 0].cpu().numpy() > 0  # –ë–∏–Ω–∞—Ä–∏–∑—É–µ–º –º–∞—Å–∫—É
                score = 0.9  # SAM2 –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç scores, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            else:
                return []
        else:
            # –ï—Å–ª–∏ masks —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
            mask = masks[0] if len(masks) > 0 else None
            score = 0.9
            
        if mask is None:
            return []
            
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–µ–≥–æ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞ –¥–ª—è –º–∞—Å–∫–∏
        y_indices, x_indices = np.where(mask)
        if len(x_indices) == 0 or len(y_indices) == 0:
            return []
            
        x_min, x_max = np.min(x_indices), np.max(x_indices)
        y_min, y_max = np.min(y_indices), np.max(y_indices)
        
        # –§–æ—Ä–º–∞—Ç: [x_min, y_min, x_max, y_max, score, class_id]
        # SAM2 –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç class_id, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 0
        bbox = [x_min, y_min, x_max, y_max, score, 0]
        return [bbox]

    def process_image_with_prompts(self, image_path, input_boxes=None, input_points=None, 
                                 input_labels=None, confidence_threshold=0.5):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ bounding boxes.
        
        Args:
            image_path: –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            input_boxes: —Å–ø–∏—Å–æ–∫ bounding boxes –≤ —Ñ–æ—Ä–º–∞—Ç–µ [[x1, y1, x2, y2]]
            input_points: —Å–ø–∏—Å–æ–∫ —Ç–æ—á–µ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [[[x, y]]]
            input_labels: –º–µ—Ç–∫–∏ —Ç–æ—á–µ–∫ (1 - –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è, 0 - –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è)
            confidence_threshold: –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            
        Returns:
            bounding_boxes: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö bounding boxes
            image_np: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ numpy
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = Image.open(image_path).convert("RGB")
        image_np = np.array(image)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        inputs = {}
        if input_boxes is not None:
            inputs['input_boxes'] = input_boxes
        if input_points is not None and input_labels is not None:
            inputs['input_points'] = input_points
            inputs['input_labels'] = input_labels
            
        inputs = self.processor(images=image, return_tensors="pt", **inputs).to(self.device)
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å–æ–∫
        original_sizes = inputs["original_sizes"].cpu()
        masks = self.processor.post_process_masks(outputs.pred_masks.cpu(), original_sizes)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º bounding boxes –∏–∑ –º–∞—Å–æ–∫
        bounding_boxes = []
        for mask_batch in masks:
            for i in range(mask_batch.shape[1]):
                mask = mask_batch[0, i].numpy() > 0
                if np.any(mask):
                    y_indices, x_indices = np.where(mask)
                    x_min, x_max = np.min(x_indices), np.max(x_indices)
                    y_min, y_max = np.min(y_indices), np.max(y_indices)
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π score, —Ç–∞–∫ –∫–∞–∫ SAM2 –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç confidence –¥–ª—è –∫–∞–∂–¥–æ–π –º–∞—Å–∫–∏
                    bbox = [x_min, y_min, x_max, y_max, 0.9, 0]
                    bounding_boxes.append(bbox)
        
        return bounding_boxes, image_np

    def process_image_automatic(self, image_path, confidence_threshold=0.5):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ä–µ–∂–∏–º–µ (–±–µ–∑ –ø—Ä–æ–º–ø—Ç–æ–≤).
        –í —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ SAM2 –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é, –ø–æ—ç—Ç–æ–º—É —ç–º—É–ª–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –¥–µ—Ç–µ–∫—Ü–∏—é.
        """
        # –î–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:
        # 1. –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—É—é —Å–µ—Ç–∫—É –ø—Ä–æ–º–ø—Ç–æ–≤
        # 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã—Ö –±–æ–∫—Å–æ–≤
        # –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏
        
        image = Image.open(image_path).convert("RGB")
        image_np = np.array(image)
        h, w = image_np.shape[:2]
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É —Ç–æ—á–µ–∫ –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤
        points = []
        step = 100
        for y in range(step, h, step):
            for x in range(step, w, step):
                points.append([x, y])
        
        if points:
            input_points = [[points]]  # –§–æ—Ä–º–∞—Ç: [[[[x1, y1], [x2, y2], ...]]]
            input_labels = [[[1] * len(points)]]  # –í—Å–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏
            
            return self.process_image_with_prompts(
                image_path, input_points=input_points, input_labels=input_labels,
                confidence_threshold=confidence_threshold
            )
        else:
            return [], image_np

def calculate_iou(box1, box2):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç Intersection over Union (IoU) –¥–≤—É—Ö bounding boxes.
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area_box1 + area_box2 - intersection
    
    return intersection / union if union > 0 else 0.0

def read_yolo_labels(label_path, img_width, img_height):
    """
    –ß–∏—Ç–∞–µ—Ç YOLO —Ñ–æ—Ä–º–∞—Ç —Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã.
    """
    boxes = []
    if not os.path.exists(label_path):
        return boxes
        
    with open(label_path, 'r') as f:
        lines = f.readlines()
        
    for line in lines:
        data = line.strip().split()
        if len(data) < 5:
            continue
            
        class_id = int(data[0])
        x_center = float(data[1]) * img_width
        y_center = float(data[2]) * img_height
        width = float(data[3]) * img_width
        height = float(data[4]) * img_height
        
        x1 = x_center - width / 2
        y1 = y_center - height / 2
        x2 = x_center + width / 2
        y2 = y_center + height / 2
        
        boxes.append([x1, y1, x2, y2, class_id])
    
    return boxes

def calculate_metrics_per_image(pred_boxes, gt_boxes, iou_threshold=0.5):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç Precision, Recall –∏ —Å—Ä–µ–¥–Ω–∏–π IoU –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    """
    if not gt_boxes and not pred_boxes:
        return {'precision': 1.0, 'recall': 1.0, 'mean_iou': 1.0, 'tp': 0, 'fp': 0, 'fn': 0}
    elif not gt_boxes:
        return {'precision': 0.0, 'recall': 0.0, 'mean_iou': 0.0, 'tp': 0, 'fp': len(pred_boxes), 'fn': 0}
    elif not pred_boxes:
        return {'precision': 0.0, 'recall': 0.0, 'mean_iou': 0.0, 'tp': 0, 'fp': 0, 'fn': len(gt_boxes)}
    
    tp = 0
    fp = 0
    fn = len(gt_boxes)
    
    ious = []
    gt_matched = [False] * len(gt_boxes)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (score)
    pred_boxes_sorted = sorted(pred_boxes, key=lambda x: x[4], reverse=True)
    
    for pred in pred_boxes_sorted:
        pred_box = pred[:4]
        best_iou = 0
        best_gt_idx = -1
        
        for i, gt in enumerate(gt_boxes):
            if gt_matched[i]:
                continue
            gt_box = gt[:4]
            iou = calculate_iou(pred_box, gt_box)
            if iou > best_iou:
                best_iou = iou
                best_gt_idx = i
        
        if best_iou >= iou_threshold and best_gt_idx != -1:
            tp += 1
            fn -= 1
            gt_matched[best_gt_idx] = True
            ious.append(best_iou)
        else:
            fp += 1
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    mean_iou = sum(ious) / len(ious) if ious else 0
    
    return {
        'precision': precision,
        'recall': recall,
        'mean_iou': mean_iou,
        'tp': tp,
        'fp': fp,
        'fn': fn,
        'ious': ious
    }

def visualize_results(image, pred_boxes, gt_boxes, output_path):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ ground truth –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.
    """
    result_image = image.copy()
    
    # –†–∏—Å—É–µ–º ground truth (–∑–µ–ª–µ–Ω—ã–π)
    for gt in gt_boxes:
        x1, y1, x2, y2, class_id = map(int, gt[:5])
        cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(result_image, f'GT_{class_id}', (x1, y1-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    
    # –†–∏—Å—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–∫—Ä–∞—Å–Ω—ã–π)
    for pred in pred_boxes:
        x1, y1, x2, y2, score, class_id = map(int, pred[:4] + [pred[4] * 100, pred[5]])
        cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 0, 255), 2)
        cv2.putText(result_image, f'P_{class_id}_{score/100:.2f}', (x1, y1-30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    if pred_boxes or gt_boxes:
        metrics = calculate_metrics_per_image(pred_boxes, gt_boxes)
        cv2.putText(result_image, f'Precision: {metrics["precision"]:.2f}', (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(result_image, f'Recall: {metrics["recall"]:.2f}', (10, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(result_image, f'IoU: {metrics["mean_iou"]:.2f}', (10, 90), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    cv2.imwrite(output_path, result_image)

def evaluate_sam2_on_dataset(images_dir, labels_dir, output_dir, use_prompts=True):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É SAM2 –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏.
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SAM2
    sam2_evaluator = SAM2InferenceWithMetrics()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    os.makedirs(output_dir, exist_ok=True)
    
    # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
    image_paths = []
    for ext in image_extensions:
        image_paths.extend(Path(images_dir).glob(ext))
        image_paths.extend(Path(images_dir).glob(ext.upper()))
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    total_metrics = {
        'precision': [],
        'recall': [],
        'mean_iou': [],
        'tp': 0,
        'fp': 0,
        'fn': 0
    }
    
    for i, image_path in enumerate(image_paths):
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}/{len(image_paths)}: {image_path.name}")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç SAM2
            if use_prompts:
                # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
                pred_boxes, image_np = sam2_evaluator.process_image_automatic(str(image_path))
            else:
                pred_boxes, image_np = sam2_evaluator.process_image_automatic(str(image_path))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º ground truth
            label_path = Path(labels_dir) / f"{image_path.stem}.txt"
            gt_boxes = read_yolo_labels(label_path, image_np.shape[1], image_np.shape[0])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = calculate_metrics_per_image(pred_boxes, gt_boxes)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            for key in ['precision', 'recall', 'mean_iou']:
                total_metrics[key].append(metrics[key])
            total_metrics['tp'] += metrics['tp']
            total_metrics['fp'] += metrics['fp']
            total_metrics['fn'] += metrics['fn']
            
            # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            output_path = Path(output_dir) / f"result_{image_path.stem}.png"
            visualize_results(image_np, pred_boxes, gt_boxes, str(output_path))
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {image_path}: {e}")
            continue
    
    # –†–∞—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
    if total_metrics['precision']:
        avg_precision = np.mean(total_metrics['precision'])
        avg_recall = np.mean(total_metrics['recall'])
        avg_iou = np.mean(total_metrics['mean_iou'])
        
        micro_precision = total_metrics['tp'] / (total_metrics['tp'] + total_metrics['fp']) if (total_metrics['tp'] + total_metrics['fp']) > 0 else 0
        micro_recall = total_metrics['tp'] / (total_metrics['tp'] + total_metrics['fn']) if (total_metrics['tp'] + total_metrics['fn']) > 0 else 0
        
        print("\n" + "="*50)
        print("–ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò SAM2:")
        print("="*50)
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(total_metrics['precision'])}")
        print(f"–°—Ä–µ–¥–Ω—è—è Precision: {avg_precision:.4f}")
        print(f"–°—Ä–µ–¥–Ω—è—è Recall: {avg_recall:.4f}")
        print(f"–°—Ä–µ–¥–Ω–∏–π IoU: {avg_iou:.4f}")
        print(f"Total TP: {total_metrics['tp']}")
        print(f"Total FP: {total_metrics['fp']}")
        print(f"Total FN: {total_metrics['fn']}")
        print(f"Micro Precision: {micro_precision:.4f}")
        print(f"Micro Recall: {micro_recall:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ —Ñ–∞–π–ª
        with open(Path(output_dir) / 'metrics.txt', 'w') as f:
            f.write("SAM2 Metrics Summary\n")
            f.write("====================\n")
            f.write(f"Processed images: {len(total_metrics['precision'])}\n")
            f.write(f"Average Precision: {avg_precision:.4f}\n")
            f.write(f"Average Recall: {avg_recall:.4f}\n")
            f.write(f"Average IoU: {avg_iou:.4f}\n")
            f.write(f"Total TP: {total_metrics['tp']}\n")
            f.write(f"Total FP: {total_metrics['fp']}\n")
            f.write(f"Total FN: {total_metrics['fn']}\n")
            f.write(f"Micro Precision: {micro_precision:.4f}\n")
            f.write(f"Micro Recall: {micro_recall:.4f}\n")
    
    return total_metrics

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—É—Ç–µ–π
    IMAGES_DIR = "Safety--&-Security-1/test/images"
    LABELS_DIR = "Safety--&-Security-1/test/labels" 
    OUTPUT_DIR = "sam2_transformers_results"
    
    # –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏
    metrics = evaluate_sam2_on_dataset(
        images_dir=IMAGES_DIR,
        labels_dir=LABELS_DIR,
        output_dir=OUTPUT_DIR
    )
# # üéØ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
# if __name__ == "__main__":
#     # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—É—Ç–µ–π (–ó–ê–ú–ï–ù–ò–¢–ï –ù–ê –°–í–û–ò!)
#     MODEL_CONFIG = "sam2.1_hiera_b+.yaml"  # –∫–æ–Ω—Ñ–∏–≥ –º–æ–¥–µ–ª–∏ SAM2
#     CHECKPOINT_PATH = "sam2.1_hiera_base_plus.pt"  # –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ SAM2
#     IMAGES_DIR = "Safety--&-Security-1/test/images"
#     LABELS_DIR = "Safety--&-Security-1/test/labels" 
#     OUTPUT_DIR = "sam2_inference_results"
#     DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
#     # –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏
#     metrics = evaluate_sam2_on_dataset(
#         model_cfg=MODEL_CONFIG,
#         checkpoint=CHECKPOINT_PATH,
#         images_dir=IMAGES_DIR,
#         labels_dir=LABELS_DIR,
#         output_dir=OUTPUT_DIR,
#         device=DEVICE
#     )